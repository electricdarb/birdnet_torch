{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torch \n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "import timm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape and batch size\n",
    "INPUT_SHAPE = (3, 640, 640)\n",
    "batch_size = 1\n",
    "\n",
    "model_name = 'yolov5n'\n",
    "onnx_model_path = f'{model_name}.onnx'\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\14135/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "C:\\Users\\14135\\miniconda3\\envs\\torch_p37\\lib\\site-packages\\matplotlib_inline\\config.py:66: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n",
      "  def _figure_formats_changed(self, name, old, new):\n",
      "YOLOv5  2022-2-23 torch 1.10.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoShape(\n",
       "  (model): DetectMultiBackend(\n",
       "    (model): Model(\n",
       "      (model): Sequential(\n",
       "        (0): Conv(\n",
       "          (conv): Conv2d(3, 16, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Conv(\n",
       "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (4): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Conv(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (6): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Conv(\n",
       "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (8): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): SPPF(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (10): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (11): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (12): Concat()\n",
       "        (13): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (14): Conv(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (15): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (16): Concat()\n",
       "        (17): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (18): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (19): Concat()\n",
       "        (20): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (21): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (22): Concat()\n",
       "        (23): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (24): Detect(\n",
       "          (m): ModuleList(\n",
       "            (0): Conv2d(64, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(128, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model =  torch.hub.load('ultralytics/yolov5', 'yolov5n') #models.mobilenet_v3_large()\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_onnx(model, path, batch_size, input_shape, simplify = True,\n",
    "                dynamic_axes =  {'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                 'output' : {0 : 'batch_size'}}):\n",
    "    dummy_input = torch.randn(batch_size, *input_shape, requires_grad = True)\n",
    "\n",
    "    torch.onnx.export(model,           # model being run\n",
    "                  dummy_input,                         # model input (or a tuple for multiple inputs)\n",
    "                  path,   # where to save the model (can be a file or file-like object)\n",
    "                  training = False,\n",
    "                  export_params = True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version = 11,          # the ONNX version to export the model to\n",
    "                  do_constant_folding = True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['images'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes = dynamic_axes)\n",
    "    \n",
    "    print(f\"model saved to {path}\")\n",
    "\n",
    "    if simplify:\n",
    "        import onnxsim\n",
    "\n",
    "        model_onnx = onnx.load(path)\n",
    "        model_onnx, check = onnxsim.simplify(\n",
    "                    model_onnx,\n",
    "                    dynamic_input_shape = True,\n",
    "                    input_shapes = {'images': [batch_size, *input_shape]})\n",
    "                    \n",
    "        assert check, 'simplified check failed'\n",
    "        onnx.save(model_onnx, onnx_model_path)\n",
    "        print('simplifed model saved')\n",
    "\n",
    "\n",
    "def load_onnx_model(path):\n",
    "    model = onnx.load(path)\n",
    "    onnx.checker.check_model(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14135\\miniconda3\\envs\\torch_p37\\lib\\site-packages\\torch\\onnx\\utils.py:310: UserWarning: It is recommended that constant folding be turned off ('do_constant_folding=False') when exporting the model in training-amenable mode, i.e. with 'training=TrainingMode.TRAIN' or 'training=TrainingMode.PRESERVE' (when model is in training mode). Otherwise, some learnable model parameters may not translate correctly in the exported ONNX model because constant folding mutates model parameters. Please consider turning off constant folding or setting the training=TrainingMode.EVAL.\n",
      "  warnings.warn(\"It is recommended that constant folding be turned off ('do_constant_folding=False') \"\n",
      "C:\\Users\\14135/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\yolo.py:57: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to yolov5n.onnx\n",
      "simplifed model saved\n"
     ]
    }
   ],
   "source": [
    "torch_to_onnx(torch_model, onnx_model_path, batch_size, INPUT_SHAPE,\n",
    "                                         dynamic_axes = {'images': {0: 'batch', 2: 'height', 3: 'width'},  # shape(1, 3, 640, 640)\n",
    "                                                        'output': {0: 'batch', 1: 'anchors'}}  # shape(1, 25200, 85)\n",
    "                                        )\n",
    "\n",
    "onnx_model = load_onnx_model(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23776/3687310740.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mINPUT_SHAPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtorch_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# compute ONNX Runtime output prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch_model' is not defined"
     ]
    }
   ],
   "source": [
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# define model input x to test onx vs torch\n",
    "inputs = torch.randn(batch_size, *INPUT_SHAPE)\n",
    "\n",
    "torch_out = torch_model(inputs)[0]\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(inputs)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0][0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.11\n",
      "[setupvars.bat] OpenVINO environment initialized\n"
     ]
    }
   ],
   "source": [
    "!\"C:\\Program Files (x86)\\Intel\\openvino_2021\\bin\\setupvars.bat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPU': <class '__main__.Versions'>}\n",
      "CPU\n",
      "MKLDNNPlugin version ......... 2.1\n",
      "Build ........... 2021.4.2-3976-0943ed67223-refs/pull/539/head\n"
     ]
    }
   ],
   "source": [
    "ie = IECore()\n",
    "device = 'CPU'#'MYRIAD'\n",
    "model = ie.read_network(model = onnx_model_path)\n",
    "versions = ie.get_versions(device)\n",
    "print(versions)\n",
    "\n",
    "print(f\"{device}\")\n",
    "print(f\"MKLDNNPlugin version ......... {versions[device].major}.{versions[device].minor}\")\n",
    "print(f\"Build ........... {versions[device].build_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "for input_key in model.input_info:\n",
    "     print(model.input_info[input_key].input_data.shape)\n",
    "\n",
    "for input_key in model.input_info:\n",
    "    input_name = input_key\n",
    "    model.input_info[input_key].precision = 'FP16' if device == 'MYRIAD' else 'FP32'\n",
    "    break\n",
    "assert (len(model.input_info.keys()) == 1 or len(model.input_info.keys()) == 2), \"Sample supports topologies only with 1 or 2 inputs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,3,640,640)\n",
      "yolov5n.onnx\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \tC:\\\\Users\\\\14135\\\\Desktop\\\\birdnet_torch\\\\yolov5n.onnx\n",
      "\t- Path for generated IR: \tC:\\\\Users\\\\14135\\\\Desktop\\\\birdnet_torch\\\\models\n",
      "\t- IR output name: \tyolov5n\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tConv_199,Conv_510,Conv_821\n",
      "\t- Input shapes: \t(1,3,640,640)\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \t255.0\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "\t- Inference Engine found in: \tC:\\Users\\14135\\miniconda3\\envs\\torch_p37\\lib\\site-packages\\openvino\n",
      "Inference Engine version: \t2021.4.2-3976-0943ed67223-refs/pull/539/head\n",
      "Model Optimizer version: \t2021.4.2-3974-e2a469a3450-releases/2021/4\n",
      "[ WARNING ] Model Optimizer and Inference Engine versions do no match.\n",
      "[ WARNING ] Consider building the Inference Engine Python API from sources or reinstall OpenVINO (TM) toolkit using \"pip install openvino==2021.4\"\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: C:\\Users\\14135\\Desktop\\birdnet_torch\\models\\yolov5n.xml\n",
      "[ SUCCESS ] BIN file: C:\\Users\\14135\\Desktop\\birdnet_torch\\models\\yolov5n.bin\n",
      "[ SUCCESS ] Total execution time: 11.44 seconds. \n",
      "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Const node 'Resize_119/Add_input_port_1/value267210789' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_141/Add_input_port_1/value270610792' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Changing Const node 'Resize_119/Add_input_port_1/value267210960' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_141/Add_input_port_1/value270611062' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n"
     ]
    }
   ],
   "source": [
    "_input_shape = str((1, *INPUT_SHAPE)).replace(' ', '')\n",
    "# jupyter bash can be pretty buggy, make sure to copy and paste input shape and model path in \n",
    "print(_input_shape)\n",
    "print(onnx_model_path) ####MAKE SURE TO COPY THIS INTO THE MODEL PATH \n",
    "# only run when model is updated \n",
    "# get conv output layers with https://netron.app webste\n",
    "\n",
    "!python \"C:\\\\Program Files (x86)\\\\Intel\\\\openvino_2021\\\\deployment_tools\\\\model_optimizer\\\\mo.py\" \\\n",
    "    --input_model \"C:\\\\Users\\\\14135\\\\Desktop\\\\birdnet_torch\\\\yolov5n.onnx\" \\\n",
    "    --output_dir \"C:\\\\Users\\\\14135\\\\Desktop\\\\birdnet_torch\\\\models\" \\\n",
    "    --input_shape (1,3,640,640) \\\n",
    "    --output Conv_199,Conv_510,Conv_821 \\\n",
    "    --data_type FP16 \\\n",
    "    --scale 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ie.read_network(model = f'models\\\\{model_name}.xml', weights = f'models\\\\{model_name}.bin')\n",
    "\n",
    "exec_net = ie.load_network(network = net, device_name = device, num_requests=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 810, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"bus.jpg\")\n",
    "print(img.shape)\n",
    "img = img[..., ::-1]\n",
    "img = cv2.resize(img, (640, 640))\n",
    "\n",
    "cv2.imshow('window', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "img = torchvision.io.read_image(\"bus.jpg\")\n",
    "img = torchvision.transforms.functional.resize(img, [640, 640])\n",
    "print(img.shape)\n",
    "img = np.expand_dims(to_numpy(img), axis = 0)\n",
    "inputs = {'images': img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = exec_net.infer(inputs=inputs)\n",
    "results = []\n",
    "for layer_name, out in outputs.items():\n",
    "    results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "YOLOV5N_ANCHORS = [\n",
    "    [10,13, 16,30, 33,23],  # P3/8\n",
    "    [30,61, 62,45, 59,119],  # P4/16\n",
    "    [116,90, 156,198, 373,326],  # P5/32 \n",
    "    ]\n",
    "\n",
    "def create_detector(num_classes = 80,\n",
    "    anchors = YOLOV5N_ANCHORS,\n",
    "    ):\n",
    "    num_outputs = num_classes + 5 # num outpus per anchor\n",
    "    num_layers = len(anchors) # number of prediction layers \n",
    "    num_anchors = len(anchors[0]) //2\n",
    "    anchors = anchors\n",
    "    grid = [np.zeros(1)] * num_layers  # init grid\n",
    "    anchor_grid = [np.zeros(1)] * num_layers # init anchor grid\n",
    "\n",
    "    def detect(predictions): # call on one image\n",
    "        z = []\n",
    "        for i, pred in enumerate(predictions): # for each prediction layer\n",
    "            batch_size, _, ny, nx = pred.shape\n",
    "\n",
    "            x = np.reshape(pred, (batch_size, num_anchors, num_outputs, ny, nx))\n",
    "            x = np.transpose(x, (0, 1, 3, 4, 2))\n",
    "\n",
    "            y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "            y[..., 0:2] = (y[..., 0:2] * 2 - 0.5 + grid[i]) # * stride[i]  # xy\n",
    "            y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * anchor_grid[i]  # wh\n",
    "\n",
    "            y = np.reshape(y, (batch_size, -1, num_outputs))\n",
    "            z.append(y)\n",
    "            \n",
    "        return np.concatenate(z, axis = 1)\n",
    "        \n",
    "    return detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 255, 80, 80)\n",
      "(1, 3, 80, 80, 85)\n",
      "(1, 3, 40, 40, 85)\n",
      "(1, 3, 20, 20, 85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18959783"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = create_detector()\n",
    "print(results[0].shape)\n",
    "x = detector(results)\n",
    "np.max(x[..., 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 80, 80, 85)\n",
      "(1, 3, 40, 40, 85)\n",
      "(1, 3, 20, 20, 85)\n",
      "(4, 1) (4, 6)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31212/3408236633.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31212/504852829.py\u001b[0m in \u001b[0;36mnon_max_suppression\u001b[1;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m# Filter by class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "x = detector(results)\n",
    "y = non_max_suppression(x, conf_thres= .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.6 ms ± 2.53 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit exec_net.infer(inputs=inputs)\n",
    "# yolov5s time: 262 ms\n",
    "# yolov5n time: 138 ms\n",
    "# yolov5s torch laptop cpu time: 442 ms \n",
    "# yolov5n torch laptop cpu time: 204 ms\n",
    "# yolov5n onnx laptop cpu time: 55 ms\n",
    "# yolov5n openvino laptop cpu: 38.6 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 ms ± 9.97 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(img)\n",
    "\n",
    "%timeit torch_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: img}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25200, 85)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(255*80*80 + 255*40*40 + 255*20*20) //85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "85*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ort_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23776/1071727834.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ort_session.run(None, ort_inputs)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\torch_p37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2362\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2363\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2364\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2365\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch_p37\\lib\\site-packages\\decorator.py\u001b[0m in \u001b[0;36mfun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch_p37\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch_p37\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m                 \u001b[0mtime_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\torch_p37\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ort_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "%timeit ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: main.py [options]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --framework {tf,caffe,mxnet,kaldi,onnx}\n",
      "                        Name of the framework used to train the input model.\n",
      "\n",
      "Framework-agnostic parameters:\n",
      "  --input_model INPUT_MODEL, -w INPUT_MODEL, -m INPUT_MODEL\n",
      "                        Tensorflow*: a file with a pre-trained model (binary\n",
      "                        or text .pb file after freezing). Caffe*: a model\n",
      "                        proto file with model weights\n",
      "  --model_name MODEL_NAME, -n MODEL_NAME\n",
      "                        Model_name parameter passed to the final create_ir\n",
      "                        transform. This parameter is used to name a network in\n",
      "                        a generated IR and output .xml/.bin files.\n",
      "  --output_dir OUTPUT_DIR, -o OUTPUT_DIR\n",
      "                        Directory that stores the generated IR. By default, it\n",
      "                        is the directory from where the Model Optimizer is\n",
      "                        launched.\n",
      "  --input_shape INPUT_SHAPE\n",
      "                        Input shape(s) that should be fed to an input node(s)\n",
      "                        of the model. Shape is defined as a comma-separated\n",
      "                        list of integer numbers enclosed in parentheses or\n",
      "                        square brackets, for example [1,3,227,227] or\n",
      "                        (1,227,227,3), where the order of dimensions depends\n",
      "                        on the framework input layout of the model. For\n",
      "                        example, [N,C,H,W] is used for Caffe* models and\n",
      "                        [N,H,W,C] for TensorFlow* models. Model Optimizer\n",
      "                        performs necessary transformations to convert the\n",
      "                        shape to the layout required by Inference Engine\n",
      "                        (N,C,H,W). The shape should not contain undefined\n",
      "                        dimensions (? or -1) and should fit the dimensions\n",
      "                        defined in the input operation of the graph. If there\n",
      "                        are multiple inputs in the model, --input_shape should\n",
      "                        contain definition of shape for each input separated\n",
      "                        by a comma, for example: [1,3,227,227],[2,4] for a\n",
      "                        model with two inputs with 4D and 2D shapes.\n",
      "                        Alternatively, specify shapes with the --input option.\n",
      "  --scale SCALE, -s SCALE\n",
      "                        All input values coming from original network inputs\n",
      "                        will be divided by this value. When a list of inputs\n",
      "                        is overridden by the --input parameter, this scale is\n",
      "                        not applied for any input that does not match with the\n",
      "                        original input of the model.\n",
      "  --reverse_input_channels\n",
      "                        Switch the input channels order from RGB to BGR (or\n",
      "                        vice versa). Applied to original inputs of the model\n",
      "                        if and only if a number of channels equals 3. Applied\n",
      "                        after application of --mean_values and --scale_values\n",
      "                        options, so numbers in --mean_values and\n",
      "                        --scale_values go in the order of channels used in the\n",
      "                        original model.\n",
      "  --log_level {CRITICAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}\n",
      "                        Logger level\n",
      "  --input INPUT         Quoted list of comma-separated input nodes names with\n",
      "                        shapes, data types, and values for freezing. The shape\n",
      "                        and value are specified as space-separated lists. The\n",
      "                        data type of input node is specified in braces and can\n",
      "                        have one of the values: f64 (float64), f32 (float32),\n",
      "                        f16 (float16), i64 (int64), i32 (int32), u8 (uint8),\n",
      "                        boolean. For example, use the following format to set\n",
      "                        input port 0 of the node `node_name1` with the shape\n",
      "                        [3 4] as an input node and freeze output port 1 of the\n",
      "                        node `node_name2` with the value [20 15] of the int32\n",
      "                        type and shape [2]: \"0:node_name1[3\n",
      "                        4],node_name2:1[2]{i32}->[20 15]\".\n",
      "  --output OUTPUT       The name of the output operation of the model. For\n",
      "                        TensorFlow*, do not add :0 to this name.\n",
      "  --mean_values MEAN_VALUES, -ms MEAN_VALUES\n",
      "                        Mean values to be used for the input image per\n",
      "                        channel. Values to be provided in the (R,G,B) or\n",
      "                        [R,G,B] format. Can be defined for desired input of\n",
      "                        the model, for example: \"--mean_values\n",
      "                        data[255,255,255],info[255,255,255]\". The exact\n",
      "                        meaning and order of channels depend on how the\n",
      "                        original model was trained.\n",
      "  --scale_values SCALE_VALUES\n",
      "                        Scale values to be used for the input image per\n",
      "                        channel. Values are provided in the (R,G,B) or [R,G,B]\n",
      "                        format. Can be defined for desired input of the model,\n",
      "                        for example: \"--scale_values\n",
      "                        data[255,255,255],info[255,255,255]\". The exact\n",
      "                        meaning and order of channels depend on how the\n",
      "                        original model was trained.\n",
      "  --data_type {FP16,FP32,half,float}\n",
      "                        Data type for all intermediate tensors and weights. If\n",
      "                        original model is in FP32 and --data_type=FP16 is\n",
      "                        specified, all model weights and biases are quantized\n",
      "                        to FP16.\n",
      "  --transform TRANSFORM\n",
      "                        Apply additional transformations. Usage: \"--transform\n",
      "                        transformation_name1[args],transformation_name2...\"\n",
      "                        where [args] is key=value pairs separated by\n",
      "                        semicolon. Examples: \"--transform LowLatency2\" or \"--\n",
      "                        transform LowLatency2[use_const_initializer=False]\"\n",
      "                        Available transformations: \"LowLatency2\"\n",
      "  --disable_fusing      Turn off fusing of linear operations to Convolution\n",
      "  --disable_resnet_optimization\n",
      "                        Turn off resnet optimization\n",
      "  --finegrain_fusing FINEGRAIN_FUSING\n",
      "                        Regex for layers/operations that won't be fused.\n",
      "                        Example: --finegrain_fusing Convolution1,.*Scale.*\n",
      "  --disable_gfusing     Turn off fusing of grouped convolutions\n",
      "  --enable_concat_optimization\n",
      "                        Turn on Concat optimization.\n",
      "  --move_to_preprocess  Move mean values to IR preprocess section\n",
      "  --extensions EXTENSIONS\n",
      "                        Directory or a comma separated list of directories\n",
      "                        with extensions. To disable all extensions including\n",
      "                        those that are placed at the default location, pass an\n",
      "                        empty string.\n",
      "  --batch BATCH, -b BATCH\n",
      "                        Input batch size\n",
      "  --version             Version of Model Optimizer\n",
      "  --silent              Prevent any output messages except those that\n",
      "                        correspond to log level equals ERROR, that can be set\n",
      "                        with the following option: --log_level. By default,\n",
      "                        log level is already ERROR.\n",
      "  --freeze_placeholder_with_value FREEZE_PLACEHOLDER_WITH_VALUE\n",
      "                        Replaces input layer with constant node with provided\n",
      "                        value, for example: \"node_name->True\". It will be\n",
      "                        DEPRECATED in future releases. Use --input option to\n",
      "                        specify a value for freezing.\n",
      "  --generate_deprecated_IR_V7\n",
      "                        Force to generate deprecated IR V7 with layers from\n",
      "                        old IR specification.\n",
      "  --static_shape        Enables IR generation for fixed input shape (folding\n",
      "                        `ShapeOf` operations and shape-calculating sub-graphs\n",
      "                        to `Constant`). Changing model input shape using the\n",
      "                        Inference Engine API in runtime may fail for such an\n",
      "                        IR.\n",
      "  --keep_shape_ops      The option is ignored. Expected behavior is enabled by\n",
      "                        default.\n",
      "  --disable_weights_compression\n",
      "                        Disable compression and store weights with original\n",
      "                        precision.\n",
      "  --progress            Enable model conversion progress display.\n",
      "  --stream_output       Switch model conversion progress display to a\n",
      "                        multiline mode.\n",
      "  --transformations_config TRANSFORMATIONS_CONFIG\n",
      "                        Use the configuration file with transformations\n",
      "                        description.\n",
      "  --legacy_ir_generation\n",
      "                        Use legacy IR serialization engine\n",
      "\n",
      "TensorFlow*-specific parameters:\n",
      "  --input_model_is_text\n",
      "                        TensorFlow*: treat the input model file as a text\n",
      "                        protobuf format. If not specified, the Model Optimizer\n",
      "                        treats it as a binary file by default.\n",
      "  --input_checkpoint INPUT_CHECKPOINT\n",
      "                        TensorFlow*: variables file to load.\n",
      "  --input_meta_graph INPUT_META_GRAPH\n",
      "                        Tensorflow*: a file with a meta-graph of the model\n",
      "                        before freezing\n",
      "  --saved_model_dir SAVED_MODEL_DIR\n",
      "                        TensorFlow*: directory with a model in SavedModel\n",
      "                        formatof TensorFlow 1.x or 2.x version.\n",
      "  --saved_model_tags SAVED_MODEL_TAGS\n",
      "                        Group of tag(s) of the MetaGraphDef to load, in string\n",
      "                        format, separated by ','. For tag-set contains\n",
      "                        multiple tags, all tags must be passed in.\n",
      "  --tensorflow_custom_operations_config_update TENSORFLOW_CUSTOM_OPERATIONS_CONFIG_UPDATE\n",
      "                        TensorFlow*: update the configuration file with node\n",
      "                        name patterns with input/output nodes information.\n",
      "  --tensorflow_use_custom_operations_config TENSORFLOW_USE_CUSTOM_OPERATIONS_CONFIG\n",
      "                        Use the configuration file with custom operation\n",
      "                        description.\n",
      "  --tensorflow_object_detection_api_pipeline_config TENSORFLOW_OBJECT_DETECTION_API_PIPELINE_CONFIG\n",
      "                        TensorFlow*: path to the pipeline configuration file\n",
      "                        used to generate model created with help of Object\n",
      "                        Detection API.\n",
      "  --tensorboard_logdir TENSORBOARD_LOGDIR\n",
      "                        TensorFlow*: dump the input graph to a given directory\n",
      "                        that should be used with TensorBoard.\n",
      "  --tensorflow_custom_layer_libraries TENSORFLOW_CUSTOM_LAYER_LIBRARIES\n",
      "                        TensorFlow*: comma separated list of shared libraries\n",
      "                        with TensorFlow* custom operations implementation.\n",
      "  --disable_nhwc_to_nchw\n",
      "                        Disables default translation from NHWC to NCHW\n",
      "\n",
      "Caffe*-specific parameters:\n",
      "  --input_proto INPUT_PROTO, -d INPUT_PROTO\n",
      "                        Deploy-ready prototxt file that contains a topology\n",
      "                        structure and layer attributes\n",
      "  --caffe_parser_path CAFFE_PARSER_PATH\n",
      "                        Path to Python Caffe* parser generated from\n",
      "                        caffe.proto\n",
      "  -k K                  Path to CustomLayersMapping.xml to register custom\n",
      "                        layers\n",
      "  --mean_file MEAN_FILE, -mf MEAN_FILE\n",
      "                        Mean image to be used for the input. Should be a\n",
      "                        binaryproto file\n",
      "  --mean_file_offsets MEAN_FILE_OFFSETS, -mo MEAN_FILE_OFFSETS\n",
      "                        Mean image offsets to be used for the input\n",
      "                        binaryproto file. When the mean image is bigger than\n",
      "                        the expected input, it is cropped. By default, centers\n",
      "                        of the input image and the mean image are the same and\n",
      "                        the mean image is cropped by dimensions of the input\n",
      "                        image. The format to pass this option is the\n",
      "                        following: \"-mo (x,y)\". In this case, the mean file is\n",
      "                        cropped by dimensions of the input image with offset\n",
      "                        (x,y) from the upper left corner of the mean image\n",
      "  --disable_omitting_optional\n",
      "                        Disable omitting optional attributes to be used for\n",
      "                        custom layers. Use this option if you want to transfer\n",
      "                        all attributes of a custom layer to IR. Default\n",
      "                        behavior is to transfer the attributes with default\n",
      "                        values and the attributes defined by the user to IR.\n",
      "  --enable_flattening_nested_params\n",
      "                        Enable flattening optional params to be used for\n",
      "                        custom layers. Use this option if you want to transfer\n",
      "                        attributes of a custom layer to IR with flattened\n",
      "                        nested parameters. Default behavior is to transfer the\n",
      "                        attributes without flattening nested parameters.\n",
      "\n",
      "Mxnet-specific parameters:\n",
      "  --input_symbol INPUT_SYMBOL\n",
      "                        Symbol file (for example, model-symbol.json) that\n",
      "                        contains a topology structure and layer attributes\n",
      "  --nd_prefix_name ND_PREFIX_NAME\n",
      "                        Prefix name for args.nd and argx.nd files.\n",
      "  --pretrained_model_name PRETRAINED_MODEL_NAME\n",
      "                        Name of a pretrained MXNet model without extension and\n",
      "                        epoch number. This model will be merged with args.nd\n",
      "                        and argx.nd files\n",
      "  --save_params_from_nd\n",
      "                        Enable saving built parameters file from .nd files\n",
      "  --legacy_mxnet_model  Enable MXNet loader to make a model compatible with\n",
      "                        the latest MXNet version. Use only if your model was\n",
      "                        trained with MXNet version lower than 1.0.0\n",
      "  --enable_ssd_gluoncv  Enable pattern matchers replacers for converting\n",
      "                        gluoncv ssd topologies.\n",
      "\n",
      "Kaldi-specific parameters:\n",
      "  --counts COUNTS       Path to the counts file\n",
      "  --remove_output_softmax\n",
      "                        Removes the SoftMax layer that is the output layer\n",
      "  --remove_memory       Removes the Memory layer and use additional inputs\n",
      "                        outputs instead\n"
     ]
    }
   ],
   "source": [
    "!python \"C:\\\\Program Files (x86)\\\\Intel\\\\openvino_2021\\\\deployment_tools\\\\model_optimizer\\\\mo.py\" --help"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "208af9daca7244bc307c856959856af182492b20a8a60620ec04a79d87aa7eb9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch_p37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
